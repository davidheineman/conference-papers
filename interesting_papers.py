FILTERED_65 = [
    "QUDsim: Quantifying Discourse Similarities in LLM-Generated Text",
    "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
    "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
    "Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a Moderately Resourced Setting",
    "The Zero Body Problem: Probing LLM Use of Sensory Language",
    "Base Models Beat Aligned Models at Randomness and Creativity",
    "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation",
    "One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in LLMs",
    "D3: A Dataset for Training Code LMs to Act Diff-by-Diff",
    "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs",
    "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation",
    "RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models",
    "LLMs as Research Tools: A Large Scale Survey of Researchers’ Usage and Perceptions",
    "Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use",
    "Rhapsody: A Dataset for Highlight Detection in Podcasts",
    "ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models",
    "Teaching Models to Understand (but not Generate) High-risk Data",
    "Fluid Language Model Benchmarking",
    "Inducing Programmatic Skills for Agentic Tasks",
    "Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models",
    "SuperBPE: Space Travel for Language Models",
    "Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge",
    "ReasonIR: Training Retrievers for Reasoning Tasks",
    "FineWeb2: One Pipeline to Scale Them All — Adapting Pre-Training Data Processing to Every Language",
    "MALT: Improving Reasoning with Multi-Agent LLM Training",
    "Post-training for Efficient Communication via Convention Formation",
    "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
    "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
    "The Unlearning Mirage: A Dynamic Framework for Evaluating LLM Unlearning",
    "EvalAgents: Discovering Implicit Evaluation Criteria from the Web",
    "Overfill: Two-Stage Models for Efficient Language Model Decoding",
    "Approximating Language Model Training Data from Weights",
    "Yourbench: Dynamic Evaluation Set Generation with LLMs",
    "From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models",
    "CLIPPER: Compression enables long-context synthetic data generation",
    "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
    "Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs",
    "Hell or High Water: Evaluating Agentic Recovery from External Failures",
    "Self-Steering Language Models",
    "Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality",
    "NoveltyBench: Evaluating Creativity and Diversity in Language Models",
    "ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data",
    "Bayesian scaling laws for in-context learning",
    "Assessing Judging Bias in Large Reasoning Models: An Empirical Study",
    "Weight ensembling improves reasoning in language models",
    "Humans overrely on overconfident language models, across languages",
    "Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs",
    "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models",
    "Reasoning Models Know When They’re Right: Probing Hidden States for Self-Verification",
    "Not All Data Are Unlearned Equally",
    "Hyperparameter Loss Surfaces Are Simple Near their Optima",
    "Hardware-Efficient Attention for Fast Decoding",
    "Breakpoint: Stress-testing systems-level reasoning in LLM agents",
    "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows",
    "$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources",
    "A Controlled Study on Long Context Extension and Generalization in LLMs",
    "Readability ≠ Learnability: Rethinking the Role of Simplicity in Training Small Language Models",
    "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation",
    "An Illusion of Progress? Assessing the Current State of Web Agents",
    "L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning",
    "Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers",
    "SmolLM2: When Smol Goes Big — Data-Centric Training of a Fully Open Small Language Model",
    "Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling",
    "ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning",
    "BEARCUBS: A benchmark for computer-using web agents",
]

FILTERED_20 = [
    "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models",
    "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation",
    "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
    "AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs",
    "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation",
    "LLMs as Research Tools: A Large Scale Survey of Researchers’ Usage and Perceptions",
    "Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use",
    "ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning ",
    "FineWeb2: One Pipeline to Scale Them All — Adapting Pre-Training Data Processing to Every ",
    "MALT: Improving Reasoning with Multi-Agent LLM Training",
    "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
    "EvalAgents: Discovering Implicit Evaluation Criteria from the Web",
    "Yourbench: Dynamic Evaluation Set Generation with LLMs",
    "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF",
    "Self-Steering Language Models",
    "Weight ensembling improves reasoning in language models",
    "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows",
    "An Illusion of Progress? Assessing the Current State of Web Agents",
    "SmolLM2: When Smol Goes Big — Data-Centric Training of a Fully Open Small Language Model",
    "BEARCUBS: A benchmark for computer-using web agents",
]