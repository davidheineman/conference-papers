Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models
- Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi
- We introduce a novel inference-time algorithm, ThoughtTracing, which uses LLMs to probabilistically trace and weight hypotheses about agents’ evolving mental states without relying on questions and ground-truth answers in benchmarks.

Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation
- Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum
- This work examines how feedback protocols (absolute scores vs. pairwise preferences) impact biases in LLM evaluations, revealing that absolute scoring is more robust to distractor features.

MLGym: A New Framework and Benchmark for Advancing AI Research Agents
- Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Mikhail Plekhanov, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Nicolaus Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu
- MLGym introduces a framework and benchmark suite for evaluating and developing large language model agents on diverse AI research tasks.

AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs
- Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia
- We propose AutoScale, which automatically predicts compute-optimal data compositions for training LLMs at the target training data scale.

LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation
- Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen
- We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation.

LLMs as Research Tools: A Large Scale Survey of Researchers’ Usage and Perceptions
- Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen Cheng, Ai-Heng Lee, Kyle Lo, Joseph Chee Chang, Amy X Zhang
- A large-scale survey of 816 researchers to study usage of LLMs in scientific research and the perception of such usage.

Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use
- Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D Manning
- We propose a synthetic data generation and RL methodology for multi-step reasoning and tool use.

ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models
- Xiao Pu, Michael Saxon, Wenyue Hua, William Yang Wang
- We evaluate the relationship between problem difficulty and token cost in reasoning models, benchmark how efficiently different models allocate tokens, and introduce a simple training-free decoding method to reduce overthinking, Thought Terminator.

FineWeb2: One Pipeline to Scale Them All — Adapting Pre-Training Data Processing to Every Language
- Guilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, Thomas Wolf
- We introduce a new pre-training dataset curation pipeline based on FineWeb that we use to create a new large multilingual dataset, FineWeb2
2

MALT: Improving Reasoning with Multi-Agent LLM Training
- Sumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, Rafael Rafailov, Philip Torr, Ivan Laptev, Fabio Pizzati, Ronald Clark, Christian Schroeder de Witt
- We introduce a multi-agent post-training approach to improve the reasoning and self-correction performance of a generator, verifier, and refinement model working together

AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories
- Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher Pal, Siva Reddy
- AgentRewardBench is a benchmark that evaluates how well Large Language Model (LLM) judges align with human preferences when assessing autonomous web agents, using over 1,200 expert-reviewed trajectories across five benchmarks

EvalAgents: Discovering Implicit Evaluation Criteria from the Web
- Manya Wadhwa, Zayne Rea Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, Greg Durrett
- We propose a novel framework EvalAgent that dynamically generates grounded, implicit evaluation criteria for a given prompt based on retrieved expert advice.

Yourbench: Dynamic Evaluation Set Generation with LLMs
- Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskaya, Thomas Wolf, Gokhan Tur, Dilek Hakkani-Tür
- a new system to generate diverse question answers from source documents ensuring maximum document coverage

Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF
- Syrine Belakaria, Joshua Kazdan, Charles Marx, Chris Cundy, Willie Neiswanger, Sanmi Koyejo, Barbara E Engelhardt, Stefano Ermon
- We provide a novel active learning method for RLHF based on the Sharpe Ratio.

Self-Steering Language Models
- Gabriel Grand, Joshua B. Tenenbaum, Vikash Mansinghka, Alexander K. Lew, Jacob Andreas
- We introduce a new approach to structuring test-time computation that uses LMs to plan and execute task-specific search procedures in a probabilistic programming language.

Weight ensembling improves reasoning in language models
- Xingyu Dang, Christina Baek, Kaiyue Wen, J Zico Kolter, Aditi Raghunathan
- Weight ensembling improves pass@k of reasoning models.

LongCodeBench: Evaluating Coding LLMs at 1M Context Windows
- Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, Tatsunori Hashimoto
- LongCodeBench, a benchmark evaluating long-context language models on real-world coding tasks—code comprehension and repair—across different context lengths up to one million tokens.

An Illusion of Progress? Assessing the Current State of Web Agents
- Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su
- We introduce Online-Mind2Web, a more diverse and realistic benchmark that includes 300 high-quality tasks from 136 popular websites across various domains.

SmolLM2: When Smol Goes Big — Data-Centric Training of a Fully Open Small Language Model
- Loubna Ben allal, Anton Lozhkov, Elie Bakouch, Gabriel Martin Blazquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Agustín Piqueres Lajarín, Hynek Kydlíček, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan Son NGUYEN, Ben Burtenshaw, Clémentine Fourrier, Haojun Zhao, Hugo Larcher, Mathieu Morlon, Cyril Zakka, Colin Raffel, Leandro Von Werra, Thomas Wolf
- SmolLM2 is a fully open 1.7B parameter LM that achieves state-of-the-art performance through multi-stage training on diverse high-quality data and is released alongside new math, code and instruction tuning datasets.

BEARCUBS: A benchmark for computer-using web agents
- Yixiao Song, Katherine Thai, Chau Minh Pham, Yapei Chang, Mazin Nadaf, Mohit Iyyer
- We introduce BEARCUBS, a benchmark of 111 information-seeking questions designed to evaluate a web agent’s ability to search, browse, and identify factual information from the web.