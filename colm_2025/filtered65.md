QUDsim: Quantifying Discourse Similarities in LLM-Generated Text
- Ramya Namuduri, Yating Wu, Anshun Asher Zheng, Manya Wadhwa, Greg Durrett, Junyi Jessy Li
- We introduce an abstraction based on linguistics theories in Questions Under Discussion (QUD) and question semantics to quantify repetitive discourse structures found in texts generated by large language models.

Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models
- Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi
- We introduce a novel inference-time algorithm, ThoughtTracing, which uses LLMs to probabilistically trace and weight hypotheses about agents’ evolving mental states without relying on questions and ground-truth answers in benchmarks.

Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving
- Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, Hongzhou Lin, Kaiyu Yang, Jia LI, Mengzhou Xia, Danqi Chen, Sanjeev Arora, Chi Jin
- Introduce Goedel-Prover, an open-source language model that achieves SOTA in automated theorem proving in Lean

Sherkala-Chat: Building a State-of-the-Art LLM for Kazakh in a Moderately Resourced Setting
- Fajri Koto, Rituraj Joshi, Nurdaulet Mukhituly, Yuxia Wang, Zhuohan Xie, Rahul Pal, Daniil Orel, Parvez Mullah, Diana Turmakhan, Maiya Goloburda, Mohammed Kamran, Samujjwal Ghosh, Bokang Jia, Jonibek Mansurov, Mukhammed Togmanov, Debopriyo Banerjee, Nurkhan Laiyk, Akhmed Sakip, Xudong Han, Ekaterina Kochmar, Alham Fikri Aji, Aaryamonvikram Singh, Alok Anil Jadhav, Satheesh Katipomu, Samta Kamboj, Monojit Choudhury, Gurpreet Gosal, Gokulakrishnan Ramakrishnan, Biswajit Mishra, Sarath Chandran, Avraham Sheinin, Natalia Vassilieva, Neha Sengupta, Preslav Nakov
- Sherkala-Chat (8B) is a state-of-the-art, instruction-tuned open LLM for Kazakh, excelling in Kazakh language tasks while remaining competitive in English.

The Zero Body Problem: Probing LLM Use of Sensory Language
- Rebecca M. M. Hicke, Sil Hamilton, David Mimno
- Popular large language models fail to replicate human use of sensory language, an important feature of storytelling.

Base Models Beat Aligned Models at Randomness and Creativity
- Peter West, Christopher Potts
- Alignment seems to hurt performance on a set of tasks that require randomness or creativity

Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation
- Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum
- This work examines how feedback protocols (absolute scores vs. pairwise preferences) impact biases in LLM evaluations, revealing that absolute scoring is more robust to distractor features.

One-shot Optimized Steering Vectors Mediate Safety-relevant Behaviors in LLMs
- Jacob Dunefsky, Arman Cohan
- Optimizing steering vectors on a single training example can yield vectors that modulate safety-relevant behavior in LLMs across wider datasets.

D3: A Dataset for Training Code LMs to Act Diff-by-Diff
- Ulyana Piterbarg, Kanishk Gandhi, Lerrel Pinto, Noah Goodman, Rob Fergus
- D3 is a dataset of 8 billion tokens of file-diff-sequence examples sampled from 850k Human-written source files, improving LM performance on code synthesis, completion, & editing.

MLGym: A New Framework and Benchmark for Advancing AI Research Agents
- Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Mikhail Plekhanov, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Nicolaus Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu
- MLGym introduces a framework and benchmark suite for evaluating and developing large language model agents on diverse AI research tasks.

AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs
- Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia
- We propose AutoScale, which automatically predicts compute-optimal data compositions for training LLMs at the target training data scale.

LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation
- Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen
- We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation.

RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models
- Juan Diego Rodriguez, Wenxuan Ding, Katrin Erk, Greg Durrett
- We reduce the generator-validator gap (a discrepancy between LLMs' generated answers and self-verification), with a ranking-based loss function, improving model consistency

LLMs as Research Tools: A Large Scale Survey of Researchers’ Usage and Perceptions
- Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen Cheng, Ai-Heng Lee, Kyle Lo, Joseph Chee Chang, Amy X Zhang
- A large-scale survey of 816 researchers to study usage of LLMs in scientific research and the perception of such usage.

Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use
- Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D Manning
- We propose a synthetic data generation and RL methodology for multi-step reasoning and tool use.

Rhapsody: A Dataset for Highlight Detection in Podcasts
- Younghan Park, Anuj Diwan, David Harwath, Eunsol Choi
- We present a dataset of 13K podcast episodes paired with segment-level highlight scores.

ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models
- Xiao Pu, Michael Saxon, Wenyue Hua, William Yang Wang
- We evaluate the relationship between problem difficulty and token cost in reasoning models, benchmark how efficiently different models allocate tokens, and introduce a simple training-free decoding method to reduce overthinking, Thought Terminator.

Teaching Models to Understand (but not Generate) High-risk Data
- Ryan Yixiang Wang, Matthew Finlayson, Luca Soldaini, Swabha Swayamdipta, Robin Jia
- A new pre-training paradigm that enables language models to understand high-risk data without learning to generate it.

Fluid Language Model Benchmarking
- Valentin Hofmann, David Heineman, Ian Magnusson, Kyle Lo, Jesse Dodge, Maarten Sap, Pang Wei Koh, Chun Wang, Hannaneh Hajishirzi, Noah A. Smith
- Fluid Benchmarking improves evaluation by adapting items to the capability level of language models.

Inducing Programmatic Skills for Agentic Tasks
- Zora Zhiruo Wang, Apurva Gandhi, Graham Neubig, Daniel Fried
- We propose ASI, Agent Skill Induction, which induces and applies skill programs from web navigation experiences without supervision, yielding improved correctness and efficiency.

Recycling the Web: A Method to Enhance Pre-training Data Quality and Quantity for Language Models
- Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, Xian Li
- We propose rewriting low-quality web documents to improve their utility, thereby increasing the availability of pre-training data for language models

SuperBPE: Space Travel for Language Models
- Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, Yejin Choi
- Superword tokenization leads to better and more efficient language models

Beyond the Reported Cutoff: Where Large Language Models Fall Short on Financial Knowledge
- Agam Shah, Liqin Ye, Sebastian Jaskowski, Wei Xu, Sudheer Chava
- This paper evaluates Large Language Models' knowledge of historical financial data, finding they know more about larger, recent companies but are also prone to hallucinations about these firms.

ReasonIR: Training Retrievers for Reasoning Tasks
- Rulin Shao, Rui Qiao, Varsha Kishore, Niklas Muennighoff, Xi Victoria Lin, Daniela Rus, Bryan Kian Hsiang Low, Sewon Min, Wen-tau Yih, Pang Wei Koh, Luke Zettlemoyer
- We present the first bi-encoder retriever specially trained to retrieve helpful documents for reasoning tasks.

FineWeb2: One Pipeline to Scale Them All — Adapting Pre-Training Data Processing to Every Language
- Guilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, Thomas Wolf
- We introduce a new pre-training dataset curation pipeline based on FineWeb that we use to create a new large multilingual dataset, FineWeb2

MALT: Improving Reasoning with Multi-Agent LLM Training
- Sumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, Rafael Rafailov, Philip Torr, Ivan Laptev, Fabio Pizzati, Ronald Clark, Christian Schroeder de Witt
- We introduce a multi-agent post-training approach to improve the reasoning and self-correction performance of a generator, verifier, and refinement model working together

Post-training for Efficient Communication via Convention Formation
- Yilun Hua, Evan Wang, Yoav Artzi
- We introduce a post-training method and new tasks to test and improve LLMs' abilities in forming conventions for efficient communication.

UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?
- Mukund Choudhary, KV Aditya Srivatsa, Gaurja Aeron, Antara Raaghavi Bhattacharya, Dang Khoa Dang Dinh, Ikhlasul Akmal Hanif, Daria Kotova, Ekaterina Kochmar, Monojit Choudhury
- The study presents a linguistic features-based annotation of Linguistics Olympiad puzzles to find LLM weaknesses; LLMs are bad at puzzles with higher morphological complexity, dissimilar to English, and when the puzzle is data constrained.

AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories
- Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher Pal, Siva Reddy
- AgentRewardBench is a benchmark that evaluates how well Large Language Model (LLM) judges align with human preferences when assessing autonomous web agents, using over 1,200 expert-reviewed trajectories across five benchmarks

The Unlearning Mirage: A Dynamic Framework for Evaluating LLM Unlearning
- Raj Sanjay Shah, Jing Huang, Keerthiram Murugesan, Nathalie Baracaldo, Diyi Yang
- A Dynamic Framework for Evaluating LLM Unlearning

EvalAgents: Discovering Implicit Evaluation Criteria from the Web
- Manya Wadhwa, Zayne Rea Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, Greg Durrett
- We propose a novel framework EvalAgent that dynamically generates grounded, implicit evaluation criteria for a given prompt based on retrieved expert advice.

Overfill: Two-Stage Models for Efficient Language Model Decoding
- Woojeong Kim, Junxiong Wang, Jing Nathan Yan, Mohamed S. Abdelfattah, Alexander M Rush
- Leveraging more compute during prefill, OverFill improves generation quality with minimal latency overhead.

Approximating Language Model Training Data from Weights
- John Xavier Morris, Junjie Oscar Yin, Woojeong Kim, Vitaly Shmatikov, Alexander M Rush
- we recover suitable training data given only model weights

Yourbench: Dynamic Evaluation Set Generation with LLMs
- Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskaya, Thomas Wolf, Gokhan Tur, Dilek Hakkani-Tür
- a new system to generate diverse question answers from source documents ensuring maximum document coverage

From Next-Token to Mathematics: The Learning Dynamics of Mathematical Reasoning in Language Models
- Shubhra Mishra, Gabriel Poesia, Noah Goodman
- We conduct the first analysis of how math reasoning skills are learned during pre- and post-training using open checkpoint and open weight models.

CLIPPER: Compression enables long-context synthetic data generation
- Chau Minh Pham, Yapei Chang, Mohit Iyyer
- We introduce CLIPPER, a compression-based approach for generating synthetic data tailored to narrative claim verification—a task that requires reasoning over a book to verify a given claim.

Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF
- Syrine Belakaria, Joshua Kazdan, Charles Marx, Chris Cundy, Willie Neiswanger, Sanmi Koyejo, Barbara E Engelhardt, Stefano Ermon
- We provide a novel active learning method for RLHF based on the Sharpe Ratio.

Can Performant LLMs Be Ethical? Quantifying the Impact of Web Crawling Opt-Outs
- Dongyang Fan, Vinko Sabolčec, Matin Ansaripour, Ayush Kumar Tarun, Martin Jaggi, Antoine Bosselut, Imanol Schlag
- We study how respecting web crawling opt-outs (robots.txt) affects LLM performance by introducing the concept of Data Compliance Gap (DCG).

Hell or High Water: Evaluating Agentic Recovery from External Failures
- Andrew Wang, Sophia Hager, Adi Asija, Daniel Khashabi, Nicholas Andrews
- Evaluating how well LLMs can find backup plans

Self-Steering Language Models
- Gabriel Grand, Joshua B. Tenenbaum, Vikash Mansinghka, Alexander K. Lew, Jacob Andreas
- We introduce a new approach to structuring test-time computation that uses LMs to plan and execute task-specific search procedures in a probabilistic programming language.

Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality
- Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier
- The paper introduces Approximate Feature Activation (AFA) and the $\varepsilon$LBO metric to address the lack of principled hyperparameter selection in top-k SAEs and to evaluate SAEs using quasi-orthogonality.

NoveltyBench: Evaluating Creativity and Diversity in Language Models
- Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, Daphne Ippolito
- We introduce a benchmark for creativity and diversity in language models.

ParaPO: Aligning Language Models to Reduce Verbatim Reproduction of Pre-training Data
- Tong Chen, Faeze Brahman, Jiacheng Liu, Niloofar Mireshghallah, Weijia Shi, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi
- Fine-tuning language models to mitigate regurgitation in open-ended generation.

Bayesian scaling laws for in-context learning
- Aryaman Arora, Dan Jurafsky, Christopher Potts, Noah Goodman
- We test the claim that in-context learning in LLMs is Bayesian, leading to a new interpretable scaling law that accurately predicts when suppressed behaviors in both toy and real-world language models will reemerge.

Assessing Judging Bias in Large Reasoning Models: An Empirical Study
- Qian Wang, Zhanzhi Lou, Zhenheng Tang, Nuo Chen, Xuandong Zhao, Wenxuan Zhang, Dawn Song, Bingsheng He
- We demonstrate that Large Reasoning Models remain susceptible to judging biases despite their advanced capabilities.

Weight ensembling improves reasoning in language models
- Xingyu Dang, Christina Baek, Kaiyue Wen, J Zico Kolter, Aditi Raghunathan
- Weight ensembling improves pass@k of reasoning models.

Humans overrely on overconfident language models, across languages
- Neil Rathi, Dan Jurafsky, Kaitlyn Zhou
- Multilingual LLMs are overconfident across languages, and that users overrely on confident responses

Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs
- Kanishk Gandhi, Ayush K Chakravarthy, Anikait Singh, Nathan Lile, Noah Goodman
- We show that language models with built-in cognitive behaviors like verification and backtracking learn better through reinforcement learning than those without.

Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language Models
- Neel Jain, Aditya Shrivastava, Chenyang Zhu, Daben Liu, Alfy Samuel, Ashwinee Panda, Anoop Kumar, Micah Goldblum, Tom Goldstein
- Introduce refusal tokens to enable control over a single model’s refusal rates and discuss desirable data properties for optimizing this approach.

Reasoning Models Know When They’re Right: Probing Hidden States for Self-Verification
- Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, He He
- Reasoning models with long chain-of-thought encode strong signals about the correctness of intermediate answers in model's hidden states, and we can use it for early exit.

Not All Data Are Unlearned Equally
- Aravind Krishnan, Siva Reddy, Marius Mosbach
- We investigate how data frequency and model scale affect the feasibility of gradient based unlearning

Hyperparameter Loss Surfaces Are Simple Near their Optima
- Nicholas Lourie, He He, Kyunghyun Cho
- We derive a theory describing the hyperparameter loss surface and yielding new statistical tools for understanding it.

Hardware-Efficient Attention for Fast Decoding
- Ted Zadouri, Hubert Strauss, Tri Dao
- Incremental decoding slows attention, we propose new hardware-efficient variants that reorganize attention to preserve parallelization and model quality while boosting speed, GPU utilization, and throughput, all with minimal cache.

Breakpoint: Stress-testing systems-level reasoning in LLM agents
- Kaivalya Hariharan, Uzay Girit, Zifan Wang, Jacob Andreas
- We introduce Breakpoint, a method of generating difficult coding tasks for models at a large scale that stress-test its system-level reasoning.

LongCodeBench: Evaluating Coding LLMs at 1M Context Windows
- Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, Tatsunori Hashimoto
- LongCodeBench, a benchmark evaluating long-context language models on real-world coding tasks—code comprehension and repair—across different context lengths up to one million tokens.

$100K or 100 Days: Trade-offs when Pre-Training with Academic Resources
- Apoorv Khandelwal, Tian Yun, Nihal V. Nayak, Jack Merullo, Stephen Bach, Chen Sun, Ellie Pavlick
- We present insights about pre-training on academic compute and a software benchmark to determine the most efficient training settings.

A Controlled Study on Long Context Extension and Generalization in LLMs
- Yi Lu, Jing Nathan Yan, Songlin Yang, Justin T Chiu, Siyu Ren, Fei Yuan, Wenting Zhao, Zhiyong Wu, Alexander M Rush
- Using a controlled protocol to systematically study long context extension methods

Readability ≠ Learnability: Rethinking the Role of Simplicity in Training Small Language Models
- Ivan Lee, Taylor Berg-Kirkpatrick
- This paper argues statistical simplicity (low n-gram diversity), not human readability, is the critical factor enabling coherence emergence in small language models trained on synthetic datasets like TinyStories.

CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation
- Anirudh Khatry, Robert Zhang, Jia Pan, Ziteng Wang, Qiaochu Chen, Greg Durrett, Isil Dillig
- We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate transpilation correctness.

An Illusion of Progress? Assessing the Current State of Web Agents
- Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su
- We introduce Online-Mind2Web, a more diverse and realistic benchmark that includes 300 high-quality tasks from 136 popular websites across various domains.

L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning
- Pranjal Aggarwal, Sean Welleck
- We propose Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that gives reasoning language models adaptive control over the length using just a prompt.

Verifying the Verifiers: Unveiling Pitfalls and Potentials in Fact Verifiers
- Wooseok Seo, Seungju Han, Jaehun Jung, Benjamin Newman, Seungwon Lim, Seungbeen Lee, Ximing Lu, Yejin Choi, Youngjae Yu
- We share three findings intended to guide future development of more robust fact verifiers.

SmolLM2: When Smol Goes Big — Data-Centric Training of a Fully Open Small Language Model
- Loubna Ben allal, Anton Lozhkov, Elie Bakouch, Gabriel Martin Blazquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Agustín Piqueres Lajarín, Hynek Kydlíček, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan Son NGUYEN, Ben Burtenshaw, Clémentine Fourrier, Haojun Zhao, Hugo Larcher, Mathieu Morlon, Cyril Zakka, Colin Raffel, Leandro Von Werra, Thomas Wolf
- SmolLM2 is a fully open 1.7B parameter LM that achieves state-of-the-art performance through multi-stage training on diverse high-quality data and is released alongside new math, code and instruction tuning datasets.

Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling
- Ben Lipkin, Benjamin LeBrun, Jacob Hoover Vigly, João Loula, David R. MacIver, Li Du, Jason Eisner, Ryan Cotterell, Vikash Mansinghka, Timothy J. O'Donnell, Alexander K. Lew, Tim Vieira
- We introduce a fast, principled, and adaptive sampler for controlled generation.

ALFA: Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning
- Shuyue Stella Li, Jimin Mun, Faeze Brahman, Pedram Hosseini, Bryceton G. Thomas, Jessica M. Sin, Bing Ren, Jonathan S. Ilgen, Yulia Tsvetkov, Maarten Sap
- Novel alignment recipe to teach LLMs perform complex goals by (1) decomposing it into more tangible attributes, (2) creating synthetic data, and (3) integrating the attributes.

BEARCUBS: A benchmark for computer-using web agents
- Yixiao Song, Katherine Thai, Chau Minh Pham, Yapei Chang, Mazin Nadaf, Mohit Iyyer
- We introduce BEARCUBS, a benchmark of 111 information-seeking questions designed to evaluate a web agent’s ability to search, browse, and identify factual information from the web.