{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ddd3f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data/main-00000-of-00001.parquet -> /Users/dhei/.cache/deviousutils\n"
     ]
    }
   ],
   "source": [
    "from deviousutils.hf import pull_parquet_from_hf\n",
    "import pandas as pd\n",
    "\n",
    "local_path = pull_parquet_from_hf(repo_id=\"davidheineman/colm-2025\", split_name=\"main\")\n",
    "df = pd.read_parquet(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5aa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 109 papers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content_TLDR</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can A Society of Generative Agents Simulate Hu...</td>\n",
       "      <td>Investigate if a multi LLM agent system can si...</td>\n",
       "      <td>Can we simulate a sandbox society with generat...</td>\n",
       "      <td>[Abe Bohan Hou, Hongru Du, Yichen Wang, Jingyu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QUDsim: Quantifying Discourse Similarities in ...</td>\n",
       "      <td>We introduce an abstraction based on linguisti...</td>\n",
       "      <td>As large language models become increasingly c...</td>\n",
       "      <td>[Ramya Namuduri, Yating Wu, Anshun Asher Zheng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hypothesis-Driven Theory-of-Mind Reasoning for...</td>\n",
       "      <td>We introduce a novel inference-time algorithm,...</td>\n",
       "      <td>Existing LLM reasoning methods have shown impr...</td>\n",
       "      <td>[Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Goedel-Prover: A Frontier Model for Open-Sourc...</td>\n",
       "      <td>Introduce Goedel-Prover, an open-source langua...</td>\n",
       "      <td>We introduce Goedel-Prover, an open-source lan...</td>\n",
       "      <td>[Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EnrichIndex: Using LLMs to Enrich Retrieval In...</td>\n",
       "      <td>EnrichIndex enriches documents offline using L...</td>\n",
       "      <td>Existing information retrieval systems excel i...</td>\n",
       "      <td>[Peter Baile Chen, Tomer Wolfson, Mike Cafarel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Fast Controlled Generation from Language Model...</td>\n",
       "      <td>We introduce a fast, principled, and adaptive ...</td>\n",
       "      <td>The dominant approach to generating from langu...</td>\n",
       "      <td>[Ben Lipkin, Benjamin LeBrun, Jacob Hoover Vig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Don’t lie to your friends: Learning what you k...</td>\n",
       "      <td>Task-level reward in a multi-agent collaborati...</td>\n",
       "      <td>To be helpful assistants, AI agents must be aw...</td>\n",
       "      <td>[Jacob Eisenstein, Reza Aghajani, Adam Fisch, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2 OLMo 2 Furious (COLM’s Version)</td>\n",
       "      <td>We release AnonModel, a family of fully open 7...</td>\n",
       "      <td>We present OLMo 2, the next generation of our ...</td>\n",
       "      <td>[Evan Pete Walsh, Luca Soldaini, Dirk Groeneve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ALFA: Aligning LLMs to Ask Good Questions A Ca...</td>\n",
       "      <td>Novel alignment recipe to teach LLMs perform c...</td>\n",
       "      <td>Large language models (LLMs) often fail to ask...</td>\n",
       "      <td>[Shuyue Stella Li, Jimin Mun, Faeze Brahman, P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>BEARCUBS: A benchmark for computer-using web a...</td>\n",
       "      <td>We introduce BEARCUBS, a benchmark of 111 info...</td>\n",
       "      <td>Modern web agents possess computer use abiliti...</td>\n",
       "      <td>[Yixiao Song, Katherine Thai, Chau Minh Pham, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Can A Society of Generative Agents Simulate Hu...   \n",
       "1    QUDsim: Quantifying Discourse Similarities in ...   \n",
       "2    Hypothesis-Driven Theory-of-Mind Reasoning for...   \n",
       "3    Goedel-Prover: A Frontier Model for Open-Sourc...   \n",
       "4    EnrichIndex: Using LLMs to Enrich Retrieval In...   \n",
       "..                                                 ...   \n",
       "104  Fast Controlled Generation from Language Model...   \n",
       "105  Don’t lie to your friends: Learning what you k...   \n",
       "106                  2 OLMo 2 Furious (COLM’s Version)   \n",
       "107  ALFA: Aligning LLMs to Ask Good Questions A Ca...   \n",
       "108  BEARCUBS: A benchmark for computer-using web a...   \n",
       "\n",
       "                                          content_TLDR  \\\n",
       "0    Investigate if a multi LLM agent system can si...   \n",
       "1    We introduce an abstraction based on linguisti...   \n",
       "2    We introduce a novel inference-time algorithm,...   \n",
       "3    Introduce Goedel-Prover, an open-source langua...   \n",
       "4    EnrichIndex enriches documents offline using L...   \n",
       "..                                                 ...   \n",
       "104  We introduce a fast, principled, and adaptive ...   \n",
       "105  Task-level reward in a multi-agent collaborati...   \n",
       "106  We release AnonModel, a family of fully open 7...   \n",
       "107  Novel alignment recipe to teach LLMs perform c...   \n",
       "108  We introduce BEARCUBS, a benchmark of 111 info...   \n",
       "\n",
       "                                              abstract  \\\n",
       "0    Can we simulate a sandbox society with generat...   \n",
       "1    As large language models become increasingly c...   \n",
       "2    Existing LLM reasoning methods have shown impr...   \n",
       "3    We introduce Goedel-Prover, an open-source lan...   \n",
       "4    Existing information retrieval systems excel i...   \n",
       "..                                                 ...   \n",
       "104  The dominant approach to generating from langu...   \n",
       "105  To be helpful assistants, AI agents must be aw...   \n",
       "106  We present OLMo 2, the next generation of our ...   \n",
       "107  Large language models (LLMs) often fail to ask...   \n",
       "108  Modern web agents possess computer use abiliti...   \n",
       "\n",
       "                                               authors  \n",
       "0    [Abe Bohan Hou, Hongru Du, Yichen Wang, Jingyu...  \n",
       "1    [Ramya Namuduri, Yating Wu, Anshun Asher Zheng...  \n",
       "2    [Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lan...  \n",
       "3    [Yong Lin, Shange Tang, Bohan Lyu, Jiayun Wu, ...  \n",
       "4    [Peter Baile Chen, Tomer Wolfson, Mike Cafarel...  \n",
       "..                                                 ...  \n",
       "104  [Ben Lipkin, Benjamin LeBrun, Jacob Hoover Vig...  \n",
       "105  [Jacob Eisenstein, Reza Aghajani, Adam Fisch, ...  \n",
       "106  [Evan Pete Walsh, Luca Soldaini, Dirk Groeneve...  \n",
       "107  [Shuyue Stella Li, Jimin Mun, Faeze Brahman, P...  \n",
       "108  [Yixiao Song, Katherine Thai, Chau Minh Pham, ...  \n",
       "\n",
       "[109 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from constants import RESEARCHERS\n",
    "\n",
    "# Check if any papers have an author in RESEARCHERS\n",
    "def has_researcher_author(authors_list):\n",
    "    if authors_list is None:\n",
    "        return False\n",
    "    return any(author in RESEARCHERS for author in authors_list)\n",
    "\n",
    "df[\"has_researcher\"] = df[\"authors\"].apply(has_researcher_author)\n",
    "\n",
    "researcher_papers = df[df[\"has_researcher\"]]\n",
    "print(f\"Found {len(researcher_papers)} papers\")\n",
    "researcher_papers[['title', 'content_TLDR', 'abstract', 'authors']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c3e0593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from interesting_papers import FILTERED_65, FILTERED_20\n",
    "\n",
    "# 1) filter by authors: 418 -> 109\n",
    "# 2) filter by reading TL;DRs: 109 -> 65\n",
    "# 3) filter by reading abstracts: 65 -> 20\n",
    "# 4) filter to papers I couldn't understand from only the abstract: 20 -> 10\n",
    "\n",
    "print(len(FILTERED_65))\n",
    "print(len(FILTERED_20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04e27936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models\n",
      "Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi\n",
      "Existing LLM reasoning methods have shown impressive capabilities across various tasks, such as solving math and coding problems. However, applying these methods to scenarios without ground-truth answers or rule-based verification methods - such as tracking the mental states of an agent - remains challenging. Inspired by the sequential Monte Carlo algorithm, we introduce ThoughtTracing, an inference-time reasoning algorithm designed to trace the mental states of specific agents by generating hypotheses and weighting them based on observations without relying on ground-truth solutions to questions in datasets. Our algorithm is modeled after the Bayesian theory-of-mind framework, using LLMs to approximate probabilistic inference over agents' evolving mental states based on their perceptions and actions. We evaluate ThoughtTracing on diverse theory-of-mind benchmarks, demonstrating significant performance improvements compared to baseline LLMs. Our experiments also reveal interesting behaviors of the recent reasoning models - e.g., o3 and R1 - on theory-of-mind, highlighting the difference of social reasoning compared to other domains.\n",
      "https://openreview.net/forum?id=yGQqTuSJPK\n",
      "\n",
      "Pairwise or Pointwise? Evaluating Feedback Protocols for Bias in LLM-Based Evaluation\n",
      "Tuhina Tripathi, Manya Wadhwa, Greg Durrett, Scott Niekum\n",
      "Large Language Models (LLMs) are widely used as proxies for human labelers in both training (Reinforcement Learning from AI Feedback) and large-scale response evaluation (LLM-as-a-judge). Alignment and evaluation are critical components in the development of reliable LLMs, and the choice of feedback protocol plays a central role in both but remains understudied. In this work, we show that the choice of feedback protocol for evaluation (absolute scores versus relative preferences) can significantly affect evaluation reliability and induce systematic biases. In the context of LLM-as-a-judge evaluation, we show that pairwise protocols are more vulnerable to **distracted evaluation**. Generator models can exploit spurious attributes (or distractor features) favored by the LLM judge, resulting in inflated scores for lower-quality outputs. We find that absolute scoring is more robust to such manipulation, producing judgments that better reflect response quality and are less influenced by distractor features. Our results demonstrate that generator models can flip preferences by embedding distractor features, skewing LLM-as-a-judge comparisons and leading to inaccurate conclusions about model quality in benchmark evaluations. **Pairwise preferences flip in about 35\\% of the cases, compared to only 9\\% for absolute scores**. We offer recommendations for choosing feedback protocols based on dataset characteristics and evaluation objectives.\n",
      "https://openreview.net/forum?id=uyX5Vnow3U\n",
      "\n",
      "MLGym: A New Framework and Benchmark for Advancing AI Research Agents\n",
      "Deepak Nathani, Lovish Madaan, Nicholas Roberts, Nikolay Bashlykov, Ajay Menon, Vincent Moens, Mikhail Plekhanov, Amar Budhiraja, Despoina Magka, Vladislav Vorotilov, Gaurav Chaurasia, Dieuwke Hupkes, Ricardo Silveira Cabral, Tatiana Shavrina, Jakob Nicolaus Foerster, Yoram Bachrach, William Yang Wang, Roberta Raileanu\n",
      "We introduce MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.\n",
      "https://openreview.net/forum?id=ryTr83DxRq\n",
      "\n",
      "AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs\n",
      "Feiyang Kang, Yifan Sun, Bingbing Wen, Si Chen, Dawn Song, Rafid Mahmood, Ruoxi Jia\n",
      "Domain reweighting is an emerging research area aimed at adjusting the relative weights of different data sources to improve the effectiveness and efficiency of LLM pre-training. We show that data mixtures that perform well at smaller scales may not retain their advantage at larger scales, challenging the existing practice of determining competitive mixtures in small-scale experiments and *directly* applying them at much larger scales. To address this, we propose AutoScale, a two-stage, scale-aware data composition framework. First, AutoScale fits a parametric model that predicts the model’s loss under different data compositions, then uses it to find an approximate best allocation at smaller, more manageable budgets. Next, leveraging a novel theoretical analysis of how optimal compositions evolve with scale, AutoScale extrapolates that composition to larger budgets without further retraining. Empirically, AutoScale accelerates convergence and improves downstream performance.\n",
      "For instance, when pre-training GPT-2 Large, it achieves a 28\\% faster perplexity reduction than baselines and up to a 38\\% speed-up over unweighted training, while yielding best-average results on various downstream tasks. Overall, our findings illustrate how domain importance shifts with training scale, underscoring the need for scale-dependent data curation in LLM training. \n",
      "Our code is open-sourced.\n",
      "https://openreview.net/forum?id=rujwIvjooA\n",
      "\n",
      "LongProc: Benchmarking Long-Context Language Models on Long Procedural Generation\n",
      "Xi Ye, Fangcong Yin, Yinghui He, Joie Zhang, Howard Yen, Tianyu Gao, Greg Durrett, Danqi Chen\n",
      "Existing benchmarks for evaluating long-context language models (LCLMs) primarily focus on long-context recall, requiring models to produce short responses based on a few critical snippets while processing thousands of irrelevant tokens.\n",
      "We introduce LongProc (Long Procedural Generation), a new benchmark that requires both the integration of highly dispersed information and long-form generation. LongProc consists of six diverse procedural generation tasks, such as extracting structured information from HTML pages into a TSV format and executing complex search procedures to create travel plans. \n",
      "These tasks challenge LCLMs by testing their ability to follow detailed procedural instructions, synthesize and reason over dispersed information, and generate structured, long-form outputs (up to 8K tokens). \n",
      "Furthermore, as these tasks adhere to deterministic procedures and yield structured outputs, they enable reliable rule-based evaluation. \n",
      "We evaluated 23 LCLMs, including instruction-tuned models and recent reasoning models, on LongProc at three difficulty levels, with the maximum number of output tokens set at 500, 2K, and 8K. \n",
      "Notably, while all tested models claim a context window size above 32K tokens, open-weight models typically falter on 2K-token tasks, and closed-source models like GPT-4o show significant degradation on 8K-token tasks.\n",
      "Reasoning models achieve stronger overall performance in long-form generation, benefiting from long CoT training.\n",
      "Further analysis reveals that LCLMs struggle to maintain long-range coherence in long-form generations.\n",
      "These findings highlight critical limitations in current LCLMs and suggest substantial room for improvement.\n",
      "https://openreview.net/forum?id=ruWC5LIMSo\n",
      "\n",
      "LLMs as Research Tools: A Large Scale Survey of Researchers’ Usage and Perceptions\n",
      "Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen Cheng, Ai-Heng Lee, Kyle Lo, Joseph Chee Chang, Amy X Zhang\n",
      "The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers actually use LLMs and why or why not. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into aspects of their research workflow. We also find that some traditionally disadvantaged groups in academia (non-white, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns. Our study provides much-needed evidence, rather than speculation, about how LLMs are currently being used as research tools.\n",
      "https://openreview.net/forum?id=p0BwJk3R1p\n",
      "\n",
      "Synthetic Data Generation and Multi-Step Reinforcement Learning for Reasoning and Tool Use\n",
      "Anna Goldie, Azalia Mirhoseini, Hao Zhou, Irene Cai, Christopher D Manning\n",
      "Reinforcement learning has been shown to improve the performance of large language models. However, traditional approaches like RLHF or RLAIF treat the problem as single-step. As focus is shifting towards solving more complex reasoning and agentic tasks, language models must take multiple steps of text generation, reasoning and environment interaction before generating a solution. We propose a synthetic data generation and RL methodology targeting multi-step optimization scenarios. This approach, called Step-Wise Reinforcement Learning (SWiRL), iteratively generates multi-step reasoning and tool use data, and then learns from that data. It employs a simple step-wise decomposition that breaks each multi-step trajectory into multiple sub-trajectories corresponding to each action by the original model. It then applies synthetic data filtering and RL optimization on these sub-trajectories. We evaluated SWiRL on a number of multi-step tool use, question answering, and mathematical reasoning tasks. Our experiments show that SWiRL outperforms baseline approaches by 21.5\\%, 12.3\\%, 14.8\\%, 11.1\\%, and 15.3\\% in relative accuracy on GSM8k, HotPotQA, CofCA, MuSiQue, and BeerQA, respectively. Excitingly, the approach exhibits generalization across tasks: for example, training only on HotPotQA (text question-answering) improves zero-shot performance on GSM8k (a math dataset) by 16.9\\%.\n",
      "https://openreview.net/forum?id=oN9STRYQVa\n",
      "\n",
      "MALT: Improving Reasoning with Multi-Agent LLM Training\n",
      "Sumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, Rafael Rafailov, Philip Torr, Ivan Laptev, Fabio Pizzati, Ronald Clark, Christian Schroeder de Witt\n",
      "Large Language Models (LLMs) often produce answers with a single chain-of-thought, which restricts their ability to explore reasoning paths or self-correct flawed outputs in complex tasks. In this paper, we introduce MALT (Multi-Agent LLM Training), a novel post-training strategy that divides the reasoning process into generation, verification, and refinement steps using a sequential pipeline of heterogeneous agents. During data generation, each agent is repeatedly sampled to form a multi-agent search tree, where final outputs are graded against ground-truth data. We then apply value iteration to propagate reward signals back to each role-conditioned model, automatically producing multi-agent post-training data without human or teacher-model supervision. Our off-policy approach allows each agent to specialize by learning from correct and incorrect trajectories, ultimately improving the end-to-end reasoning chain. On MATH, GSM8K, and CSQA, MALT surpasses the same baseline LLM with relative improvements of 15.66%, 7.42%, and 9.40%. It also generalizes to more challenging benchmarks, marking an early advance in multi-agent cooperative training.\n",
      "https://openreview.net/forum?id=jXP9bgFack\n",
      "\n",
      "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories\n",
      "Xing Han Lù, Amirhossein Kazemnejad, Nicholas Meade, Arkil Patel, Dongchan Shin, Alejandra Zambrano, Karolina Stanczak, Peter Shaw, Christopher Pal, Siva Reddy\n",
      "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io\n",
      "https://openreview.net/forum?id=fQcUZMPIvu\n",
      "\n",
      "EvalAgents: Discovering Implicit Evaluation Criteria from the Web\n",
      "Manya Wadhwa, Zayne Rea Sprague, Chaitanya Malaviya, Philippe Laban, Junyi Jessy Li, Greg Durrett\n",
      "Evaluation of language model outputs on structured writing tasks is typically conducted with a number of desirable criteria presented to human evaluators or large language models (LLMs). For instance, on a prompt like \"Help me draft an academic talk on coffee intake vs research productivity\", a model response may be evaluated for criteria like accuracy and coherence. However, high-quality responses should do more than just satisfy basic task requirements. An effective response to this query should include quintessential features of an academic talk, such as a compelling opening, clear research questions, and a takeaway. To help identify these implicit criteria, we introduce EvalAgent, a novel framework designed to automatically uncover nuanced and task-specific criteria. EvalAgent first mines expert-authored online guidance. It then uses this evidence to propose diverse, long-tail evaluation criteria that are grounded in reliable external sources. Our experiments demonstrate that the grounded criteria produced by EvalAgent are often implicit (not directly stated in the user's prompt), yet specific (high degree of lexical precision). Further, EvalAgent criteria are often not satisfied by initial responses but they are actionable, such that responses can be refined to satisfy them. Finally, we show that combining LLM-generated and EvalAgent criteria uncovers more human-valued criteria than using LLMs alone.\n",
      "https://openreview.net/forum?id=erGpkHCybv\n",
      "\n",
      "Yourbench: Dynamic Evaluation Set Generation with LLMs\n",
      "Sumuk Shashidhar, Clémentine Fourrier, Alina Lozovskaya, Thomas Wolf, Gokhan Tur, Dilek Hakkani-Tür\n",
      "Large language models (LLMs) have rapidly outpaced traditional evaluation methodologies, with static benchmarks suffering from saturation, contamination, and domain-specificity limitations while human evaluation remains prohibitively expensive. We present YourBench, an open-source framework that transforms this evaluation paradigm by enabling automated generation of reliable, contamination-free benchmarks directly from user-provided documents without human annotation. To validate our approach, we successfully reproduce the challenging MMLU-Pro benchmark across 86 models spanning 400M to 405B parameters, achieving remarkable Pearson correlations of 0.91-0.99 while generating entirely novel questions for under $15 per model. This demonstrates that dynamically generated evaluations can match the discriminative power of expert-curated benchmarks while eliminating contamination risks. YourBench enables researchers to create domain-specific benchmarks in minutes rather than months. We demonstrate applications in agriculture, personalized education, and RAG training that were previously infeasible. By releasing the YourBench library, Tempora-0325 dataset, 150K+ generated QA pairs, and all evaluation traces, we provide the community with a practical solution to the challenge of keeping pace with rapidly evolving model capabilities.\n",
      "https://openreview.net/forum?id=bkWERVKzuP\n",
      "\n",
      "Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF\n",
      "Syrine Belakaria, Joshua Kazdan, Charles Marx, Chris Cundy, Willie Neiswanger, Sanmi Koyejo, Barbara E Engelhardt, Stefano Ermon\n",
      "Reinforcement learning from human feedback (RLHF) has become a cornerstone of the training and alignment pipeline for large language models (LLMs). Recent advances, such as direct preference optimization (DPO), have simplified the preference learning step. However, collecting preference data remains a challenging and costly process, often requiring expert annotation. This cost can be mitigated by carefully selecting the data points presented for annotation. In this work, we propose an active learning approach to efficiently select prompt and preference pairs using a risk assessment strategy based on the Sharpe Ratio. \n",
      "To address the challenge of unknown preferences prior to annotation, our method evaluates the gradients of all potential preference annotations to assess their impact on model updates. These gradient-based evaluations enable risk assessment of data points regardless of the annotation outcome. By leveraging the DPO loss derivations, we derive a \\emph{closed-form expression} for computing these Sharpe ratios on a per-tuple basis, ensuring our approach remains both \\emph{tractable} and \\emph{computationally efficient}.  We also introduce two variants of our method, each making different assumptions about prior information. Experimental results demonstrate that our method outperforms the baseline by up to 5\\% in win rates against the chosen completion with limited human preference data across several language models and real-world datasets.\n",
      "https://openreview.net/forum?id=a6xzTqMUFQ\n",
      "\n",
      "Self-Steering Language Models\n",
      "Gabriel Grand, Joshua B. Tenenbaum, Vikash Mansinghka, Alexander K. Lew, Jacob Andreas\n",
      "While test-time reasoning enables language models (LMs) to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its *abstract structure*—both how to verify solutions and *how to search* for them. This paper introduces DisCIPL, a method for “self-steering” LMs where a *Planner model* generates a task-specific *inference program* that is executed by a population of *Follower models*. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B or Qwen3-1.7B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. Our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.\n",
      "https://openreview.net/forum?id=XvCBtm5PgF\n",
      "\n",
      "Weight ensembling improves reasoning in language models\n",
      "Xingyu Dang, Christina Baek, Kaiyue Wen, J Zico Kolter, Aditi Raghunathan\n",
      "We investigate a pitfall during the training of reasoning models where the diversity of generations begins to collapse, leading to suboptimal test-time scaling. Notably, Pass@1 reliably improves during supervised finetuning (SFT), but Pass@k rapidly deteriorates. Surprisingly, a simple intervention of interpolating the weights of the latest SFT checkpoint with an early checkpoint, otherwise known as WiSE-FT, almost completely recovers Pass@k while also improving Pass@1. The WiSE-FT variant achieves better test-time scaling (Best@k, majority vote) and achieves superior results with less data when tuned further by reinforcement learning. Finally, we note that WiSE-FT provides complementary gains across performance metrics that is not achievable by diversity-inducing decoding strategies alone, like temperature scaling. We formalize a \\emph{bias-variance tradeoff} of Pass@k with respect to the expectation and variance of Pass@1 over the test distribution. We find that WiSE-FT can reduce bias and variance simultaneously, while temperature scaling and possibly other decoding strategies face an inherent tradeoff between decreasing variance with increasing bias.\n",
      "https://openreview.net/forum?id=S2IKxulLT1\n",
      "\n",
      "LongCodeBench: Evaluating Coding LLMs at 1M Context Windows\n",
      "Stefano Rando, Luca Romani, Alessio Sampieri, Luca Franco, John Yang, Yuta Kyuragi, Fabio Galasso, Tatsunori Hashimoto\n",
      "Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. \n",
      "The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce **LongCodeBench** (**LCB**), a benchmark to test LLM coding abilities in long-context scenarios. \n",
      "Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (**LongCodeQA**) and bug fixing (**LongSWE-Bench**) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.\n",
      "https://openreview.net/forum?id=GFPoM8Ylp8\n",
      "\n",
      "An Illusion of Progress? Assessing the Current State of Web Agents\n",
      "Tianci Xue, Weijian Qi, Tianneng Shi, Chan Hee Song, Boyu Gou, Dawn Song, Huan Sun, Yu Su\n",
      "As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85\\% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.\n",
      "https://openreview.net/forum?id=6jZi4HSs6o\n",
      "\n",
      "SmolLM2: When Smol Goes Big — Data-Centric Training of a Fully Open Small Language Model\n",
      "Loubna Ben allal, Anton Lozhkov, Elie Bakouch, Gabriel Martin Blazquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Agustín Piqueres Lajarín, Hynek Kydlíček, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan Son NGUYEN, Ben Burtenshaw, Clémentine Fourrier, Haojun Zhao, Hugo Larcher, Mathieu Morlon, Cyril Zakka, Colin Raffel, Leandro Von Werra, Thomas Wolf\n",
      "Large language models, while groundbreaking, are computationally expensive and difficult to deploy in resource-constrained settings. To address this challenge, small language models have emerged, but their performance critically depends on the quality and composition of the pretraining datasets—yet many recent models, such as Qwen2.5-1.5B and Llama3.2-1B, remain opaque about their training data, limiting reproducibility and scientific understanding. In this paper, we document and publicly release SmolLM2, a fully transparent state-of-the-art ``small'' (1.7 billion parameter) language model (LM), along with its training datasets and code. To attain strong performance, we overtrain SmolLM2 on 11 trillion tokens of data using a multi-stage training process that mixes web text with specialized math, code, and instruction-following data. We additionally curate and release new specialized datasets (FineMath, Stack-Edu, and SmolTalk) at stages where we found existing datasets to be problematically small or low-quality. To inform our design decisions, we perform both small-scale ablations and a manual refinement process that updates the dataset mixing rates at each stage based on the performance at the previous one. Ultimately, we demonstrate that SmolLM2 outperforms other recent small LMs including Qwen2.5-1.5B, Llama3.2-1B, and Falcon3-1.6B. By releasing our model, datasets, and code, we aim to facilitate future research on LM development as well as applications of small LMs.\n",
      "https://openreview.net/forum?id=3JiCl2A14H\n",
      "\n",
      "BEARCUBS: A benchmark for computer-using web agents\n",
      "Yixiao Song, Katherine Thai, Chau Minh Pham, Yapei Chang, Mazin Nadaf, Mohit Iyyer\n",
      "Modern web agents possess computer use abilities that allow them to interact with webpages by sending commands to a virtual keyboard and mouse. While such agents have considerable potential to assist human users with complex tasks, evaluating their capabilities in real-world settings poses a major challenge. To this end, we introduce BEARCUBS, a “smallbut mighty” benchmark of 111 information-seeking questions designed to evaluate a web agent’s ability to search, browse, and identify factual information from the web. Unlike prior web agent benchmarks, solving BEARCUBS requires (1) accessing live web content rather than synthetic or simulated pages, which captures the unpredictability of real-world web interactions; and (2) performing a broad range of multimodal interactions (e.g., video understanding, 3D navigation) that cannot be bypassed via text-based workarounds. Each question in BEARCUBS has a corresponding short, unambiguous answer and a human-validated browsing trajectory, allowing for transparent evaluation of agent performance and strategies. A human study confirms that BEARCUBS questions are solvable but non-trivial (84.7% human accuracy), revealing domain knowledge gaps and overlooked details as common failure points. We find that ChatGPT Agent significantly outperforms other computer-using agents with an overall accuracy of 65.8% (compared to e.g., Operator’s 23.4%), showcasing substantial progress in tasks involving real computer use, such as playing web games and navigating 3D environments. Nevertheless, closing the gap to human performance requires improvements in areas like fine control, complex data filtering, and execution speed. To facilitate future research, BEARCUBS will be updated periodically to replace invalid or contaminated questions, keeping the benchmark fresh for future generations of web agents.\n",
      "https://openreview.net/forum?id=0JzWiigkUy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in df.iterrows():\n",
    "    idx, entry = row\n",
    "    if entry['title'] in FILTERED_20:\n",
    "        print(entry['title'])\n",
    "        print(', '.join(entry['authors']))\n",
    "        print(entry['abstract'])\n",
    "        print(entry['openreview_url'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea30790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from deviousutils.hf import push_parquet_to_hf\n",
    "\n",
    "# researcher_papers.to_parquet(\"researcher_papers.parquet\")\n",
    "# push_parquet_to_hf(\n",
    "#     file_path=\"researcher_papers.parquet\",\n",
    "#     hf_dataset_name=\"davidheineman/colm-2025\",\n",
    "#     subset_name=\"selected\",\n",
    "#     private=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07a7d643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers in SESSION_1 that are also in FILTERED_65: 14\n",
      "- Breakpoint: Stress-testing systems-level reasoning in LLM agents\n",
      "- Hell or High Water: Evaluating Agentic Recovery from External Failures\n",
      "- Inducing Programmatic Skills for Agentic Tasks\n",
      "- BEARCUBS: A benchmark for computer-using web agents\n",
      "- RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models\n",
      "- Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs\n",
      "- Assessing Judging Bias in Large Reasoning Models: An Empirical Study\n",
      "- ThoughtTerminator: Benchmarking, Calibrating, and Mitigating Overthinking in Reasoning Models\n",
      "- Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling\n",
      "- Self-Steering Language Models\n",
      "- QUDsim: Quantifying Discourse Similarities in LLM-Generated Text\n",
      "- Rhapsody: A Dataset for Highlight Detection in Podcasts\n",
      "- $100K or 100 Days: Trade-offs when Pre-Training with Academic Resources\n",
      "- AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs\n",
      "\n",
      "Papers in SESSION_1 that are also in FILTERED_20: 3\n",
      "- BEARCUBS: A benchmark for computer-using web agents\n",
      "- Self-Steering Language Models\n",
      "- AutoScale: Scale-Aware Data Mixing for Pre-Training LLMs\n"
     ]
    }
   ],
   "source": [
    "from poster_sessions import SESSION_1\n",
    "\n",
    "session_1_in_filtered_65 = [paper for paper in SESSION_1 if paper in FILTERED_65]\n",
    "print(f\"Papers in SESSION_1 that are also in FILTERED_65: {len(session_1_in_filtered_65)}\")\n",
    "for paper in session_1_in_filtered_65:\n",
    "    print(f\"- {paper}\")\n",
    "\n",
    "print()\n",
    "\n",
    "session_1_in_filtered_20 = [paper for paper in SESSION_1 if paper in FILTERED_20]\n",
    "print(f\"Papers in SESSION_1 that are also in FILTERED_20: {len(session_1_in_filtered_20)}\")\n",
    "for paper in session_1_in_filtered_20:\n",
    "    print(f\"- {paper}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
