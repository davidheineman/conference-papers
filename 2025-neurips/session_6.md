### [A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection](https://neurips.cc//virtual/2025/poster/121505)
*Gaku Morio, Harri Rowlands, Dominik Stammbach, Christopher D Manning, Peter Henderson*

Companies spend large amounts of money on public relations campaigns to project a positive brand image.However, sometimes there is a mismatch between what they say and what they do. Oil \& gas companies, for example, are accused of ``greenwashing'' with imagery of climate-friendly initiatives.Understanding the framing, and changes in framing, at scale can help better understand the goals and nature of public relation campaigns.To address this, we introduce a benchmark dataset of expert-annotated video ads obtained from Facebook and YouTube.The dataset provides annotations for 13 framing types for more than 50 companies or advocacy groups across 21 countries.Our dataset is especially designed for the evaluation of vision-language models (VLMs), distinguishing it from past text-only framing datasets.Baseline experiments show some promising results, while leaving room for improvement for future work: GPT-4.1 can detect environmental messages with 79\% F1 score, while our best model only achieves 46\% F1 score on identifying framing around green innovation.We also identify challenges that VLMs must address, such as implicit framing, handling  videos of various lengths, or implicit cultural backgrounds.Our dataset contributes to research in multimodal analysis of strategic communication in the energy sector.


### [Better Language Model Inversion by Compactly Representing Next-Token Distributions](https://neurips.cc//virtual/2025/poster/119029)
*Murtaza Nazir, Matthew Finlayson, John Morris, Xiang Ren, Swabha Swayamdipta*

Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model’s system message. We propose a new method – prompt inversion from logprob sequences (PILS) – that recovers hidden prompts by gleaning clues from the model’s next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2–3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5–27% higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings suggest that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.


### [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://neurips.cc//virtual/2025/poster/119714)
*Andrew Wang, Elisa Nguyen, Runshi Yang, Juhan Bae, Sheila McIlraith, Roger Grosse*

Training data attribution (TDA) provides insights into which training data is responsible for a learned model behavior. Gradient-based TDA methods such as influence functions and unrolled differentiation both involve a computation that resembles an inverse Hessian-vector product (iHVP), which is difficult to approximate efficiently. We introduce an algorithm (ASTRA) which uses the EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP approximation for TDA. ASTRA is easy to tune, requires fewer iterations than Neumann series iterations, and is more accurate than EKFAC-based approximations. Using ASTRA, we show that improving the accuracy of the iHVP approximation can significantly improve TDA performance.


### [CAT: Content-Adaptive Image Tokenization](https://neurips.cc//virtual/2025/poster/117055)
*Junhong Shen, Kushal Tirumala, Michihiro Yasunaga, Ishan Misra, Luke Zettlemoyer, LILI YU, Chunting Zhou*

Most existing image tokenizers encode images into a fixed number of tokens or patches, overlooking the inherent variability in image complexity and introducing unnecessary computate overhead for   simpler images. To address this, we propose Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens. We design (1) a caption-based evaluation system that leverages LLMs to predict content complexity and determine the optimal compression ratio for an image, and (2) a novel nested VAE architecture that performs variable-rate compression in a single model.Trained on images with  varying   complexity, CAT achieves an average of 15% reduction in rFID across seven detail-rich datasets containing text, humans, and complex textures. On natural image datasets like ImageNet and COCO, it  reduces token   usage by 18% while maintaining high-fidelity reconstructions.   We further evaluate CAT on two downstream tasks.  For image classification, CAT consistently improves top-1 accuracy across  five datasets spanning diverse domains. For image generation, it boosts training throughput by 23% on ImageNet, leading to more efficient learning and  improved FIDs over fixed-token baselines.


### [Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality](https://neurips.cc//virtual/2025/poster/115039)
*Alex Fang, Hadi Pouransari, Matt Jordan, Alexander Toshev, Vaishaal Shankar, Ludwig Schmidt, Tom Gunter*

Data filtering has become a powerful tool for improving model performance while reducing computational cost. However, as large language model compute budgets continue to grow, the limited data volume provided by heavily filtered and deduplicated datasets will become a practical constraint. In efforts to better understand how to proceed, we study model performance at various compute budgets and across multiple pre-training datasets created through data filtering and deduplication. We find that, given appropriate modifications to the training recipe, repeating existing aggressively filtered datasets for up to ten epochs can outperform training on the ten times larger superset for a single epoch across multiple compute budget orders of magnitude. While this finding relies on repeating the dataset for many epochs, we also investigate repeats within these datasets at the document level. We find that not all documents within a dataset are equal, and we can create better datasets relative to a token budget by explicitly manipulating the counts of individual documents. We conclude by arguing that even as large language models scale, data filtering remains an important direction of research.


### [Enhancing Training Data Attribution with Representational Optimization](https://neurips.cc//virtual/2025/poster/119133)
*Weiwei Sun, Haokun Liu, Nikhil Kandpal, Colin Raffel, Yiming Yang*

Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical rigor, their computational costs make them impractical for large-scale applications. Representation-based attribution methods are more efficient, relying on similarity computations between examples in some representation space, but they often lack task-aware and model-specific optimization, limiting their accuracy. To address these challenges, we propose AirRep, a novel representation-based approach that enhances representation quality through task-driven optimization of a representation encoding model.Furthermore, we extend this method beyond single-sample attribution using an attention-based pooling mechanism to effectively estimate the collective influence of groups of samples.Experiments on instruction tuning of large language models demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient. Further analysis highlights its robustness, including generalization to new data and new TDA tasks.


### [Establishing Best Practices in Building Rigorous Agentic Benchmarks](https://neurips.cc//virtual/2025/poster/121769)
*Yuxuan Zhu, Tengjun Jin, Yada Pruksachatkun, Andy Zhang, Shu Liu, Sasha Cui, Sayash Kapoor, Shayne Longpre, Kevin Meng, Rebecca Weiss, Fazl Barez, Rahul Gupta, Jwala Dhamala, Jacob Merizian, Mario Giulianelli, Harry Coppock, Cozmin Ududec, Antony Kellermann, Jasjeet Sekhon, Jacob Steinhardt, Sarah Schwettmann, Arvind Narayanan, Matei A Zaharia, Ion Stoica, Percy Liang, Daniel Kang*

Benchmarks are essential tools for quantitatively tracking progress in AI. As AI agents become increasingly capable of solving complex, real-world tasks, researchers and practitioners have introduced agentic benchmarks that evaluate agents based on task outcomes. However, we show that using outcome-based design suboptimally can misrepresent the true capabilities of agents. For example, SWE-bench-Verified uses insufficient test cases, while τ -bench counts empty responses as successful. Such flaws can lead to under- or overestimation agents’ performance by up to 100% in relative terms. To address this issue, we introduce the Agentic Benchmark Checklist (ABC), a set of benchmark development guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.


### [Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models](https://neurips.cc//virtual/2025/poster/116649)
*Wancong Zhang, Uladzislau Sobal, Kyunghyun Cho, Randall Balestriero, Tim G. J. Rudner, Yann LeCun*

A long-standing goal in AI is to develop agents capable of solving diverse tasks across a range of environments, including those never seen during training. Two dominant paradigms address this challenge: (i) reinforcement learning (RL), which learns policies via trial and error, and (ii) optimal control, which plans actions using a known or learned dynamics model. However, their comparative strengths in the offline setting—where agents must learn from reward-free trajectories—remain underexplored. In this work, we systematically evaluate RL and control-based methods on a suite of navigation tasks, using offline datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and employ it for planning. We investigate how factors such as data diversity, trajectory quality, and environment variability influence the performance of these approaches. Our findings reveal that model-free RL excels when abundant, high-quality data is available, while model-based planning demonstrates superior generalization to novel layouts, better trajectory stitching, and greater data efficiency. Notably, planning with a latent dynamics model proves to be a strong approach for handling suboptimal offline data and adapting to diverse environment configurations.


### [Sparta Alignment: Collectively Aligning Multiple Language Models through Combat](https://neurips.cc//virtual/2025/poster/116133)
*Tintin Jiang, Wenxuan Ding, Shangbin Feng, Greg Durrett, Yulia Tsvetkov*

We propose Sparta Alignment, an algorithm to collectively align multiple LLMs through competition and combat. To complement a single model's lack of diversity in generation and biases in evaluation, multiple LLMs form a 'sparta tribe' to compete against each other in fulfilling instructions while serving as judges for the competition of others. For each iteration, one instruction and two models are selected for a duel, the other models evaluate the two responses, and their evaluation scores are aggregated through a adapted elo-ranking based reputation system, where winners/losers of combat gain/lose weight in evaluating others. The peer-evaluated combat results then become preference pairs where the winning response is preferred over the losing one, and all models learn from these preferences at the end of each iteration. Sparta Alignment enables the self-evolution of multiple LLMs in an iterative and collective competition process. Extensive experiments demonstrate that Sparta Alignment outperforms initial models and 4 self-alignment baselines across 10 out of 12 tasks and datasets with 7.0\% average improvement. Further analysis reveals that Sparta Alignment generalizes more effectively to unseen tasks and leverages the expertise diversity of participating models to produce more logical, direct and informative outputs.